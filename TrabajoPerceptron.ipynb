{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL_nQfBrOu8b",
        "outputId": "7854eef6-eccb-4eb1-eef5-972394817413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos finales: [-0.26352501  0.00548712]\n",
            "Umbral final: -0.0933732854222849\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definir la función de activación (función escalón)\n",
        "def activation_function(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Inicializar los pesos y el umbral en valores aleatorios pequeños\n",
        "def initialize_weights(n):\n",
        "    return np.random.uniform(-1, 1, n), np.random.uniform(-1, 1)\n",
        "\n",
        "# Definir el algoritmo de entrenamiento\n",
        "def train_perceptron(X, D, learning_rate, epochs):\n",
        "    # Inicializar pesos y umbral\n",
        "    W, theta = initialize_weights(X.shape[1])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(X)):\n",
        "            # Paso 3: Propagar y calcular Y\n",
        "            X_i = X[i]\n",
        "            net_input = np.dot(X_i, W) - theta\n",
        "            Y = activation_function(net_input)\n",
        "\n",
        "            # Paso 4: Calcular el error\n",
        "            error = D[i] - Y\n",
        "\n",
        "            # Paso 5: Retropropagar y actualizar los pesos y el umbral\n",
        "            W += learning_rate * error * X_i\n",
        "            theta -= learning_rate * error\n",
        "\n",
        "            # Acumular el error total para evaluación\n",
        "            total_error += abs(error)\n",
        "\n",
        "        # Si el error total es cero, el entrenamiento se detiene\n",
        "        if total_error == 0:\n",
        "            print(f\"Entrenamiento completado en la época {epoch+1}\")\n",
        "            break\n",
        "    return W, theta\n",
        "\n",
        "# Datos de ejemplo (X: entradas, D: salidas deseadas)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Entradas\n",
        "D = np.array([0, 1, 1, 0])  # Salidas deseadas para una compuerta OR\n",
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "\n",
        "# Entrenar el perceptrón\n",
        "W, theta = train_perceptron(X, D, learning_rate, epochs)\n",
        "\n",
        "print(\"Pesos finales:\", W)\n",
        "print(\"Umbral final:\", theta)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P1PA3YL_P2Pu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}